{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b47d489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f97d9cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, context_len, embedding_dim, head_dim):\n",
    "        super().__init__()\n",
    "        self.contex_len = context_len\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_dim = head_dim\n",
    "        assert embedding_dim % head_dim == 0\n",
    "\n",
    "        self.q = nn.Linear(embedding_dim, head_dim, bias=False)\n",
    "        self.k = nn.Linear(embedding_dim, head_dim, bias=False)\n",
    "        self.v = nn.Linear(embedding_dim, head_dim, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(context_len, context_len)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape B, T, E\n",
    "        q = self.q(x)  # B, T, H\n",
    "        k = self.k(x)  # B, T, H\n",
    "        v = self.v(x)  # B, T, H\n",
    "        w = q @ k.transpose(-2, -1) * self.head_dim**-0.5  # B, T, T\n",
    "        w = w.masked_fill(self.tril == 0, float(\"-inf\"))\n",
    "        w = torch.softmax(w, dim=-1)\n",
    "        out = w @ v\n",
    "        return out  # B, T, H (head_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d4ff2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHH(nn.Module):\n",
    "    def __init__(self, context_len, embedding_dim, head_dim):\n",
    "        super().__init__()\n",
    "        self.context_len = context_len\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_dim = head_dim\n",
    "        assert embedding_dim % head_dim == 0\n",
    "        self.num_heads = embedding_dim // head_dim\n",
    "        self.heads = nn.ModuleList(\n",
    "            [Head(context_len, embedding_dim, head_dim) for _ in range(self.num_heads)]\n",
    "        )\n",
    "        self.projection_layer = nn.Linear(self.embedding_dim, self.embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        results = [head(x) for head in self.heads]\n",
    "        results = torch.cat(results, dim=-1)\n",
    "        return self.projection_layer(results)\n",
    "\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 4 * embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embedding_dim, embedding_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ffn(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, context_len, embedding_dim, head_dim):\n",
    "        super().__init__()\n",
    "        self.context_len = context_len\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_dim = head_dim\n",
    "        self.mhh = MHH(context_len, embedding_dim, head_dim)\n",
    "        self.ffn = FeedForwardNetwork(embedding_dim)\n",
    "        self.layer_norm_1 = nn.LayerNorm(embedding_dim)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mhh(self.layer_norm_1(x))\n",
    "        x = x + self.ffn(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a893491",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, context_len, embedding_dim, head_dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.head_dim = head_dim\n",
    "        self.context_len = context_len\n",
    "        self.embedding_sim = embedding_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_embeddings = nn.Embedding(context_len, embedding_dim)\n",
    "\n",
    "        self.transformers = nn.Sequential(\n",
    "            TransformerBlock(context_len, embedding_dim, head_dim),\n",
    "            TransformerBlock(context_len, embedding_dim, head_dim),\n",
    "            TransformerBlock(context_len, embedding_dim, head_dim),\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "        )\n",
    "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        x = self.embedding(x) + self.positional_embeddings(\n",
    "            torch.arange(x.size(1), device=x.device)\n",
    "        )\n",
    "        out = self.output_layer(self.transformers(x))\n",
    "        if targets is not None:\n",
    "            # out dim (B, T, E)\n",
    "            out = out.flatten(0, 1)\n",
    "            targets = targets.flatten()\n",
    "            loss = nn.functional.cross_entropy(out, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "        return out, loss\n",
    "\n",
    "    def generate(self, idx, horizon_limit):\n",
    "        for _ in range(horizon_limit):\n",
    "            idx_trimmed = idx[:, -self.context_len :]\n",
    "            logits, _ = self(idx_trimmed)\n",
    "            activations = logits[:, -1, :]\n",
    "            probs = torch.softmax(activations, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_next], dim=-1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cb00ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"shake.txt\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6b8bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "vocab_size = len(vocab)\n",
    "batch_size = 4\n",
    "context_len = 8\n",
    "embedding_dim = 32\n",
    "head_dim = 16\n",
    "train_split_pct = 0.9\n",
    "train_split_idx = int(train_split_pct * len(text))\n",
    "train_text = text[:train_split_idx]\n",
    "test_text = text[train_split_idx:]\n",
    "max_steps = 10000\n",
    "val_check = 500\n",
    "val_steps = 50\n",
    "\n",
    "char2idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx2char = {idx: char for char, idx in char2idx.items()}\n",
    "\n",
    "\n",
    "def encode(x):\n",
    "    return [char2idx[char] for char in x]\n",
    "\n",
    "\n",
    "def decode(idxs):\n",
    "    return \"\".join([idx2char[idx] for idx in idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efbfb7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakeDataset(Dataset):\n",
    "    def __init__(self, text: str, context_len: int):\n",
    "        self.text = text\n",
    "        self.context_len = context_len\n",
    "        self.encoded_text = torch.tensor(encode(text))\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        x = self.encoded_text[idx : idx + context_len]\n",
    "        y = self.encoded_text[idx + 1 : idx + context_len + 1]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_text) - self.context_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eef7a7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ShakeDataset(train_text, context_len)\n",
    "test_dataset = ShakeDataset(test_text, context_len)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8adda010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = TransformerLanguageModel(vocab_size, context_len, embedding_dim, head_dim)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86b804b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  4.312373113632202\n",
      "Test loss:  4.346557111740112\n",
      "Train loss:  2.610311346054077\n",
      "Test loss:  2.628956799507141\n",
      "Train loss:  2.5060336208343506\n",
      "Test loss:  2.5220720601081847\n",
      "Train loss:  2.4326512694358824\n",
      "Test loss:  2.4677705144882203\n",
      "Train loss:  2.37625497341156\n",
      "Test loss:  2.349957518577576\n",
      "Train loss:  2.374982142448425\n",
      "Test loss:  2.3944062995910644\n",
      "Train loss:  2.2466498684883116\n",
      "Test loss:  2.2545091271400453\n",
      "Train loss:  2.289176394939423\n",
      "Test loss:  2.303149197101593\n",
      "Train loss:  2.25638267993927\n",
      "Test loss:  2.235528516769409\n",
      "Train loss:  2.221574754714966\n",
      "Test loss:  2.3426869988441466\n",
      "Train loss:  2.2510094237327576\n",
      "Test loss:  2.1914745283126833\n",
      "Train loss:  2.219421772956848\n",
      "Test loss:  2.254181044101715\n",
      "Train loss:  2.2224785184860227\n",
      "Test loss:  2.2982584738731386\n",
      "Train loss:  2.175561077594757\n",
      "Test loss:  2.2401065540313723\n",
      "Train loss:  2.2030136251449584\n",
      "Test loss:  2.2291086173057555\n",
      "Train loss:  2.149005799293518\n",
      "Test loss:  2.1746397399902344\n",
      "Train loss:  2.20053594827652\n",
      "Test loss:  2.180664808750153\n",
      "Train loss:  2.1265653944015503\n",
      "Test loss:  2.1481179714202883\n",
      "Train loss:  2.1042109513282776\n",
      "Test loss:  2.1641138768196106\n",
      "Train loss:  2.0886558222770693\n",
      "Test loss:  2.1958186531066897\n"
     ]
    }
   ],
   "source": [
    "steps = 0\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for x, y in itertools.islice(dataloader, val_steps):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        _, loss = model(x, y)\n",
    "        losses.append(loss.item())\n",
    "    model.train()\n",
    "    return sum(losses) / len(losses) if losses else float(\"inf\")\n",
    "\n",
    "\n",
    "model.train()\n",
    "while True:\n",
    "    for batch in train_dataloader:\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits, loss = model(x, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if steps >= max_steps:\n",
    "            break\n",
    "        if steps % val_check == 0:\n",
    "            print(f\"Step {steps} Train loss: \", evaluate(model, train_dataloader, device))\n",
    "            print(f\"Step {steps] Test loss: \", evaluate(model, test_dataloader, device))\n",
    "        steps += 1\n",
    "    else:\n",
    "        continue\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "477de226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[500][0].unsqueeze(0).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36d1e052",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = model.generate(train_dataset[500][0].unsqueeze(0).to(\"mps\"), 500)\n",
    "seq = seq.flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd790289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" citizen\\nTo ad And to stank.\\nSo roffir couc not as  ane sold sels\\nGown besmparrsefuld mingervince :\\nO.\\n\\nCach sard'y?\\n\\nYour'll not Go by meappirnts have gend in forese that Loud unnow'd the ose dises!nows, artard is a cend bise hard cands mestll a he shoun Ted tconk the\\nke spatind me cearel!\\n\\nKIRITES Io Car Rorf die her athe for discatuped is not had badbrudd lold, ar om? on that grup hanct will mars thit the to tust, swi warr and have they ow his dion beath:\\nLoth, him my salpes move!\\nHis steedecr; of an\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c927f45e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toyllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
